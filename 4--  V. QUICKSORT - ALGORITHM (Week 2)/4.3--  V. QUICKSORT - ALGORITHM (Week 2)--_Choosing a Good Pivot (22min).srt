1
00:00:01,140 --> 00:00:02,760
So let's review the story so far.

2
00:00:02,760 --> 00:00:04,460
We've been discussing
the QuickSort algorithm.

3
00:00:04,460 --> 00:00:06,570
Here again is the high level description.

4
00:00:06,570 --> 00:00:09,660
So QuickSort you call two
subroutines first and

5
00:00:09,660 --> 00:00:11,920
then you make two recursive calls.

6
00:00:11,920 --> 00:00:15,070
So the first subroutine, Choose Pivot,
we haven't discussed yet at all.

7
00:00:15,070 --> 00:00:18,050
That'll be one of the main
topics of this video.

8
00:00:18,050 --> 00:00:21,970
But the job of the Choose Pivot subroutine
is to somehow select one of the n

9
00:00:21,970 --> 00:00:24,920
elements in the input array
to act as a pivot element.

10
00:00:24,920 --> 00:00:26,370
Then what does it mean to be a pivot?

11
00:00:26,370 --> 00:00:28,830
Well that comes into play
in the second subroutine.

12
00:00:28,830 --> 00:00:32,340
The partition subroutine, which we did
discuss quite a bit in a previous video.

13
00:00:32,340 --> 00:00:35,580
So what partition does is it rearranges
the elements in the input array, so

14
00:00:35,580 --> 00:00:37,080
that it has the following property.

15
00:00:37,080 --> 00:00:40,120
So that the pivot p winds up
in its rightful position.

16
00:00:40,120 --> 00:00:43,350
That is, it's to the right of all
of the elements less than it, and

17
00:00:43,350 --> 00:00:45,950
it's to the left of all of
the elements bigger than it.

18
00:00:45,950 --> 00:00:48,590
The stuff less than it's to
the left in some jumbled order,

19
00:00:48,590 --> 00:00:51,608
the stuff bigger than it's to
the right in some jumbled order.

20
00:00:51,608 --> 00:00:53,520
That's what's,
is listed here as the first part and

21
00:00:53,520 --> 00:00:55,970
the second part of the partitioned array.

22
00:00:55,970 --> 00:00:58,210
Now once you've done this partitioning,
you're good to go.

23
00:00:58,210 --> 00:01:02,264
You just recursively solve you
recursively sort the first part,

24
00:01:02,264 --> 00:01:03,730
[INAUDIBLE] the right order,

25
00:01:03,730 --> 00:01:07,050
you call QuickSort again to recursively
sort the right part, and bingo.

26
00:01:07,050 --> 00:01:08,350
The entire array is sorted.

27
00:01:08,350 --> 00:01:09,660
You don't need a combine step.

28
00:01:09,660 --> 00:01:10,865
You don't need a merge step.

29
00:01:10,865 --> 00:01:14,318
Moreover recall in a previous video we
saw that the partition array can be

30
00:01:14,318 --> 00:01:15,903
implemented in linear time, and

31
00:01:15,903 --> 00:01:19,560
moreover it works in place with
essentially no additional storage.

32
00:01:19,560 --> 00:01:23,130
We also in an optional video formally
prove the correctness of QuickSort.

33
00:01:23,130 --> 00:01:26,266
And remember QuickSort is
independent of how you implement

34
00:01:26,266 --> 00:01:27,903
the Choose Pivot subroutine.

35
00:01:27,903 --> 00:01:30,773
So what we're going to do now,
is discuss the running time of

36
00:01:30,773 --> 00:01:35,061
the QuickSort algorithm, and this is where
the choice of the pivot is very important.

37
00:01:37,159 --> 00:01:39,657
So what everybody should be
wondering about at this point, is,

38
00:01:39,657 --> 00:01:41,110
is QuickSort a good algorithm?

39
00:01:41,110 --> 00:01:42,260
Does it run fast?

40
00:01:42,260 --> 00:01:43,400
The bar's pretty high,

41
00:01:43,400 --> 00:01:47,381
we already have MergeSort which is a very
excellent practical and logging algorithm.

42
00:01:52,289 --> 00:01:56,132
The key point to realize at this
juncture is that we are not currently in

43
00:01:56,132 --> 00:02:00,330
a position to discuss the running
time of the QuickSort algorithm.

44
00:02:00,330 --> 00:02:02,590
The reason is we do not
have enough information.

45
00:02:02,590 --> 00:02:06,840
The running time of QuickSort depends
crucially on how you choose the pivot.

46
00:02:06,840 --> 00:02:10,206
It depends crucially on
the quality of the pivot chosen.

47
00:02:15,759 --> 00:02:19,030
You'd be right to wonder what
I mean by a pivot's quality.

48
00:02:19,030 --> 00:02:22,698
And basically what I mean is a pivot is
good if it splits the partitioned array

49
00:02:22,698 --> 00:02:25,320
into roughly two equal sized sub-problems.

50
00:02:25,320 --> 00:02:28,996
And it's bad, it's of low quality if
we get very unbalanced sub-problems.

51
00:02:28,996 --> 00:02:33,199
So to understand both what I mean in the
ramifications of having good quality and

52
00:02:33,199 --> 00:02:36,915
bad quality pivots, let's walk
through a couple of quiz questions.

53
00:02:39,725 --> 00:02:43,391
This first quiz question is meant to
explore a sort of worse case execution of

54
00:02:43,391 --> 00:02:45,190
the QuickSort algorithm.

55
00:02:45,190 --> 00:02:48,870
What happens when you choose pivots
that are very poorly suited for

56
00:02:48,870 --> 00:02:50,700
the particular input array?

57
00:02:50,700 --> 00:02:55,500
Maybe more specific, suppose we use the
most naive choose pivot implementation,

58
00:02:55,500 --> 00:02:58,180
like we're discussing
in the partition video.

59
00:02:58,180 --> 00:03:01,790
So remember here we just pluck out
the first element of the array, and

60
00:03:01,790 --> 00:03:03,420
we use that as the pivot.

61
00:03:03,420 --> 00:03:06,410
So suppose that's how we implement
the ChoosePivot sub-routine and

62
00:03:06,410 --> 00:03:11,180
more over, suppose that the input
array to QuickSort is an array

63
00:03:11,180 --> 00:03:13,250
that's already in sorted order.

64
00:03:13,250 --> 00:03:16,140
So for example, if it just had the numbers
one through eight, it would be one,

65
00:03:16,140 --> 00:03:18,750
two, three, four, five,
six, seven, eight in order.

66
00:03:18,750 --> 00:03:22,390
My question for you is,
what is the running time of

67
00:03:22,390 --> 00:03:27,156
this recursive QuickSort algorithm
on an already sorted array,

68
00:03:27,156 --> 00:03:31,764
if we always use the first element
of a sub-array as the pivot?

69
00:03:34,086 --> 00:03:37,100
Okay, so this is slightly tricky but
actually a very important question.

70
00:03:37,100 --> 00:03:39,360
So the answer is the fourth one.

71
00:03:39,360 --> 00:03:43,170
So it turns out that QuickSort,
if you pass it an already sorted array and

72
00:03:43,170 --> 00:03:47,580
you're using the first element as pivot
elements, it runs in quadratic time.

73
00:03:47,580 --> 00:03:51,410
And remember, for a sorting algorithm,
quadratic is bad.

74
00:03:51,410 --> 00:03:55,400
It's bad in the sense that we can do
better, MergeSort runs in time n log n,

75
00:03:55,400 --> 00:03:57,206
which is much better than n squared.

76
00:03:57,206 --> 00:03:58,810
And if we are happy with
an n squared running time,

77
00:03:58,810 --> 00:04:03,200
we wouldn't have to resort to these sort
of relatively exotic sorting algorithms.

78
00:04:03,200 --> 00:04:05,200
We could just use insertion sort and
we'd be fine.

79
00:04:05,200 --> 00:04:07,500
We'd get that same quadratic running time.

80
00:04:07,500 --> 00:04:09,090
Okay, so now I owe you an explanation.

81
00:04:09,090 --> 00:04:12,368
Why is it that QuickSort can
actually run in quadratic time,

82
00:04:12,368 --> 00:04:16,175
in this unlucky case of being
passed already sorted input array.

83
00:04:20,042 --> 00:04:22,804
Well to understand, let's think
about what pivot gets chosen and

84
00:04:22,804 --> 00:04:24,926
what are the ramifications
of that pivot choice for

85
00:04:24,926 --> 00:04:28,610
how the array gets partitioned, and
then what the recursion looks like.

86
00:04:28,610 --> 00:04:33,660
So, lets just think of the array as being
the numbers one through n in sorted order.

87
00:04:33,660 --> 00:04:34,900
What is going to be our pivot?

88
00:04:34,900 --> 00:04:37,953
Well by definition, we're choosing
the first element of the Pivot, so

89
00:04:37,953 --> 00:04:39,370
the Pivots are just going to be one.

90
00:04:39,370 --> 00:04:41,850
Now we're going to invoke
the partition subroutine, and

91
00:04:41,850 --> 00:04:45,130
if you go back to the pseudo code of the
partition sub routine, you'll notice that

92
00:04:45,130 --> 00:04:48,455
if we pass an already sorted array,
it's going to do essentially nothing.

93
00:04:48,455 --> 00:04:51,560
because it's just going to advance
the index in j until it falls off at

94
00:04:51,560 --> 00:04:52,230
the end of the array, and

95
00:04:52,230 --> 00:04:56,380
it's just going to return back to us this
same array that it was pathed as input.

96
00:04:56,380 --> 00:05:00,360
So partition sub routine if
given an already sorted array,

97
00:05:00,360 --> 00:05:02,240
returns an already sorted array.

98
00:05:02,240 --> 00:05:03,938
Okay?
So we have just the pivot one in

99
00:05:03,938 --> 00:05:05,264
the first position, and

100
00:05:05,264 --> 00:05:09,196
then the numbers two through n in order
in the remainder of the positions.

101
00:05:11,488 --> 00:05:15,467
So if we draw our usual picture of
what a partitioned array looks like,

102
00:05:15,467 --> 00:05:18,233
with everything less than
the pivot to the left,

103
00:05:18,233 --> 00:05:21,500
everything bigger than
the pivot to the right.

104
00:05:21,500 --> 00:05:26,500
Well, since nothing is less than the
pivot, this stuff is going to be empty.

105
00:05:26,500 --> 00:05:28,475
This will not exist.

106
00:05:28,475 --> 00:05:33,179
And to the right of the pivot,
this will have length n minus 1, and

107
00:05:33,179 --> 00:05:35,625
moreover it will still be sorted.

108
00:05:37,458 --> 00:05:38,793
So once partition completes,

109
00:05:38,793 --> 00:05:42,740
we go back to the outer call of QuickSort
which then calls itself recursively twice.

110
00:05:42,740 --> 00:05:45,505
Now in this case one of
the recursive calls is just vacuous.

111
00:05:45,505 --> 00:05:47,160
There's just and empty array,
there's nothing to do.

112
00:05:47,160 --> 00:05:49,400
So really there's only one recursive call,
and

113
00:05:49,400 --> 00:05:53,350
that happens on a problem
of size only one less.

114
00:05:53,350 --> 00:05:56,531
So this is about the most unbalanced
split we could possibly see.

115
00:05:56,531 --> 00:05:59,871
All right, well one side has 0 elements,
one size n minus 1.

116
00:05:59,871 --> 00:06:02,121
So don't really get any worse than that.

117
00:06:03,765 --> 00:06:06,510
And this is going to keep happening over,
and over, and over again.

118
00:06:06,510 --> 00:06:08,840
We're going to recurse on
the numbers two through n,

119
00:06:08,840 --> 00:06:11,980
we're going to choose the first element,
the two as the pivot.

120
00:06:11,980 --> 00:06:15,310
Again we'll feed it to partition, we'll
get back the exact same subarray that

121
00:06:15,310 --> 00:06:18,350
we handed it in, we'll get through
the numbers 2 through n in sorted order.

122
00:06:18,350 --> 00:06:21,730
We exclude the pivot two,
recurse on the numbers three through n.

123
00:06:21,730 --> 00:06:24,410
A subarray of length n minus two.

124
00:06:24,410 --> 00:06:28,560
The next recursion level we recurse on
an array of size of length n minus 3,

125
00:06:28,560 --> 00:06:32,980
then n minus 4, then n minus 5 and
so on, until finally, after,

126
00:06:32,980 --> 00:06:38,050
at a recursion depth of n roughly, we
would get down to just the last element n.

127
00:06:38,050 --> 00:06:42,190
The base case kicks in, and
we return that and QuickSort completes.

128
00:06:42,190 --> 00:06:45,280
So that's how QuickSort is going to
execute on this particular input with

129
00:06:45,280 --> 00:06:46,781
these particular pivot choices.

130
00:06:46,781 --> 00:06:50,004
So what running time does that give to us?

131
00:06:50,004 --> 00:06:53,789
Well, the first observation is that,
you know, in each recursive call,

132
00:06:53,789 --> 00:06:56,688
we do have to invoke
the partition subroutine.

133
00:06:56,688 --> 00:07:00,800
And the partition subroutine does
look at every element in the array it

134
00:07:00,800 --> 00:07:01,930
is passed as input.

135
00:07:01,930 --> 00:07:06,070
So if we pass partition in array of
linked K, it's going to do atleast K

136
00:07:06,070 --> 00:07:08,410
operations because it looks at
each element at least once.

137
00:07:08,410 --> 00:07:11,870
So the runtime is going
to be bounded below,

138
00:07:11,870 --> 00:07:15,200
by the work we do in the outermost call,
which is on an array of

139
00:07:15,200 --> 00:07:19,780
linked n plus the amount we do in the
second, a level of recursion, which is on

140
00:07:19,780 --> 00:07:25,250
a sub-array of linked minus 1 plus n
minus 2, plus blah-blah-blah-blah-blah.

141
00:07:25,250 --> 00:07:30,002
All the way down to plus 1 for
the very last level of recursion.

142
00:07:30,002 --> 00:07:32,742
So this is a lower band
on our running time, and

143
00:07:32,742 --> 00:07:35,145
this is already theta of n squared.

144
00:07:36,786 --> 00:07:41,013
So one easy way to see why this sum n
plus n minus 1 plus et cetera, et cetera,

145
00:07:41,013 --> 00:07:45,420
leads to a bound of n squared is to just
focus on the first half of the terms.

146
00:07:45,420 --> 00:07:49,800
So the first n over two terms in the sum
are all of magnitude at least n over 2.

147
00:07:49,800 --> 00:07:52,470
So the sum is at least N squared over 4.

148
00:07:52,470 --> 00:07:55,760
It's also evident that this
sum is at most n squared.

149
00:07:55,760 --> 00:07:56,840
So overall,

150
00:07:56,840 --> 00:08:00,580
the running time of Quicksort on this
bad input is going to be quadratic.

151
00:08:02,170 --> 00:08:05,580
Now having understood what the worst case
performance of the Quicksort algorithm is,

152
00:08:05,580 --> 00:08:08,380
let's move on to discuss
it's best case running time.

153
00:08:08,380 --> 00:08:11,550
Now we don't generally care about the best
case performance of algorithms for

154
00:08:11,550 --> 00:08:12,560
it's own sake.

155
00:08:12,560 --> 00:08:15,640
The reason that we want to think
about Quicksort in the best case.

156
00:08:15,640 --> 00:08:18,910
First of all, it'll give us better
intuition for how the algorithm works.

157
00:08:18,910 --> 00:08:21,131
Second of all,
it'll draw a line in the sand.

158
00:08:21,131 --> 00:08:24,796
Its average case running time certainly
can't be better than the best case, so

159
00:08:24,796 --> 00:08:27,257
this will give us a target for
what we're shooting for

160
00:08:27,257 --> 00:08:29,297
in our subsequent mathematical analysis.

161
00:08:31,472 --> 00:08:32,690
So what was the best case?

162
00:08:32,690 --> 00:08:35,220
What is the highest quality
pivot we could hope for?

163
00:08:35,220 --> 00:08:39,150
Well again, we think of the quality of
a pivot as the amount of balance that it

164
00:08:39,150 --> 00:08:41,010
provides between the two subproblems.

165
00:08:41,010 --> 00:08:45,460
So ideally we chose a pivot which gave
us two sub-problems both of size and

166
00:08:45,460 --> 00:08:47,130
over two or less.

167
00:08:47,130 --> 00:08:48,820
And there's a name for

168
00:08:48,820 --> 00:08:52,360
the element that would give us
that perfectly balanced split.

169
00:08:52,360 --> 00:08:54,640
It's the median element of the array,
okay?

170
00:08:54,640 --> 00:08:57,290
The element where exactly half of
the elements are less than it, and

171
00:08:57,290 --> 00:08:59,350
half the elements are bigger than it.

172
00:08:59,350 --> 00:09:03,060
That would give us essentially a perfect
50/50 split of the input array.

173
00:09:03,060 --> 00:09:04,300
So here's the question.

174
00:09:04,300 --> 00:09:07,170
Suppose we had some input and
we ran QuickSort and

175
00:09:07,170 --> 00:09:11,520
everything just worked out in our favor in
the, magically in the best possible way.

176
00:09:11,520 --> 00:09:14,400
That is in every single
recursive implication of

177
00:09:14,400 --> 00:09:17,340
Quicksort on any sub-array
of the original input array,

178
00:09:17,340 --> 00:09:22,110
suppose we happen to get as
our pivot the median element.

179
00:09:22,110 --> 00:09:27,080
That is supposing every single recursive
call we wind up getting a perfect 50 50

180
00:09:27,080 --> 00:09:29,950
split of the input array
before we recurse.

181
00:09:29,950 --> 00:09:34,122
This question asks you to analyze
the running time of this algorithm in

182
00:09:34,122 --> 00:09:36,181
this magical best case scenario.

183
00:09:38,673 --> 00:09:42,144
So the answer to this third to
this question is the third option.

184
00:09:42,144 --> 00:09:44,896
The answer is it runs in and login time.

185
00:09:47,372 --> 00:09:48,187
Why is that?

186
00:09:48,187 --> 00:09:51,228
Well the reason is that then the
recurrence which governs the running time

187
00:09:51,228 --> 00:09:54,123
of quick sort is, exactly matches
the recurrence that governs,

188
00:09:54,123 --> 00:09:56,690
the merge sort running time,
which we already know is n log n.

189
00:09:59,190 --> 00:10:03,360
That is the running time QuickSort
requires in this magically special case on

190
00:10:03,360 --> 00:10:07,300
an array of length n, well as usually,
you have a recurrence in two parts.

191
00:10:07,300 --> 00:10:09,940
There's the work that gets done
by the recursive calls and

192
00:10:09,940 --> 00:10:11,620
then there's the work that gets done now.

193
00:10:11,620 --> 00:10:15,239
Now by assumption, we wind up
picking the median as the pivot.

194
00:10:15,239 --> 00:10:18,285
So, there's going to be
two recursive calls,

195
00:10:18,285 --> 00:10:21,873
each of which will be on an input
of size at most N over 2.

196
00:10:21,873 --> 00:10:27,728
And we can write this, this is
because the pivot equals the median.

197
00:10:27,728 --> 00:10:29,732
So this is not true for
Quicksort in general.

198
00:10:29,732 --> 00:10:32,468
It's only true in this magical case
where the pivot is the median.

199
00:10:34,104 --> 00:10:36,759
So that's what gets done by
the two recursive calls and

200
00:10:36,759 --> 00:10:39,600
then how much work do we do
outside of the recursive calls?

201
00:10:39,600 --> 00:10:41,792
Well, we have to do the two
pivots sub routine and

202
00:10:41,792 --> 00:10:45,119
I guess strictly speaking I haven't
said how that was implemented.

203
00:10:45,119 --> 00:10:48,593
But let's assume that choose pivot does
only a linear amount of work, and then as

204
00:10:48,593 --> 00:10:52,810
we've seen the partition sub routine only
does a linear amount of work as well.

205
00:10:52,810 --> 00:10:56,511
So let's say O of n for
work outside of the recursive calls.

206
00:10:59,274 --> 00:11:00,435
And what do we know?

207
00:11:00,435 --> 00:11:03,860
We know this implies,
say by using the master method or

208
00:11:03,860 --> 00:11:07,367
by just using the exact same
argument as for MergeSort.

209
00:11:07,367 --> 00:11:10,208
This gives us a running
time bound of n log n.

210
00:11:11,999 --> 00:11:15,471
And then again something I haven't really
been emphasizing though which is true,

211
00:11:15,471 --> 00:11:17,363
is that actually we can
right theta of n log n.

212
00:11:17,363 --> 00:11:20,915
And that's because in the recurrence, in
fact we know that the work done outside of

213
00:11:20,915 --> 00:11:22,890
the recursive causes exactly theta of n,
okay?

214
00:11:22,890 --> 00:11:26,220
Partition needs really linear time,
not just bit O of n time.

215
00:11:26,220 --> 00:11:29,056
In fact the work done outside
the recursive calls is theta of n,

216
00:11:29,056 --> 00:11:32,482
that's because the partition sub routine
does indeed look at every entry in

217
00:11:32,482 --> 00:11:33,894
the array that it passed.

218
00:11:33,894 --> 00:11:37,554
And as a result, we didn't really discuss
this so much in the master method but

219
00:11:37,554 --> 00:11:40,880
as I mentioned in passing if you
have recurrences which are tight in

220
00:11:40,880 --> 00:11:44,318
this sense then the, the results
of the master method can also be

221
00:11:44,318 --> 00:11:46,553
strengthened to be theta
instead of just big O.

222
00:11:48,613 --> 00:11:50,207
But those are just some extra details.

223
00:11:50,207 --> 00:11:52,847
The upshot of this quiz is
that even in the best case,

224
00:11:52,847 --> 00:11:56,687
even if we magically get perfect pivots
throughout the entire trajectory of

225
00:11:56,687 --> 00:11:59,689
QuickSort, the best we can hope for
is a n log n upper bound.

226
00:11:59,689 --> 00:12:01,271
It's not going to get
any better than that.

227
00:12:01,271 --> 00:12:06,294
So the question is, how do we have a
principled way of choosing pivots so that

228
00:12:06,294 --> 00:12:11,489
we get this best case or something like
it, this best case n log n running time?

229
00:12:11,489 --> 00:12:13,467
So that's what that problem
that we have to solve next.

230
00:12:14,869 --> 00:12:19,191
So the last couple of quizzes have
identified super important question as

231
00:12:19,191 --> 00:12:21,745
far as the implementation of QuickSort,

232
00:12:21,745 --> 00:12:24,500
which is how are we going to
choose these pivots?

233
00:12:24,500 --> 00:12:25,906
Right?
We now know that they have a big

234
00:12:25,906 --> 00:12:27,980
influence on the running
time of our algorithm.

235
00:12:27,980 --> 00:12:30,605
It could be bad as n squared or
as good as n log n.

236
00:12:30,605 --> 00:12:32,898
And we really want to
be on the n log n side.

237
00:12:32,898 --> 00:12:37,047
So the key question, how to choose pivots.

238
00:12:41,645 --> 00:12:45,950
And Quicksort is the first killer
application we're going to see of the idea

239
00:12:45,950 --> 00:12:50,925
of randomized algorithms, that is allowing
your algorithms to flip coins in the code,

240
00:12:50,925 --> 00:12:54,441
so that you get some kind of
good performance on average.

241
00:12:58,172 --> 00:13:02,577
So the big idea, is random pivots.

242
00:13:05,742 --> 00:13:10,440
By which I mean for every time we
recursively call QuickSort and we, or pass

243
00:13:10,440 --> 00:13:15,587
some subarray of length K, among the K
candidate pivot elements in the sub array,

244
00:13:15,587 --> 00:13:20,900
we're going to choose each one with eq,
equally likely with probability 1 over K.

245
00:13:20,900 --> 00:13:23,015
And we're going to make a new
random choice every time we

246
00:13:23,015 --> 00:13:26,490
have a recursive call, and then we're
just going to see how the algorithm does.

247
00:13:26,490 --> 00:13:28,720
So this is our first example
of a randomized algorithm.

248
00:13:28,720 --> 00:13:32,180
This is an algorithm where if you
feed it exactly the same input,

249
00:13:32,180 --> 00:13:35,090
it will actually run differently
on different executions.

250
00:13:35,090 --> 00:13:39,280
And that's because there's randomness
internal to the code of the algorithm.

251
00:13:39,280 --> 00:13:42,420
Now it's not necessarily intuitive
that randomization should have any

252
00:13:42,420 --> 00:13:46,700
purpose in computation,
in software design, and algorithm design.

253
00:13:46,700 --> 00:13:49,816
But in fact, and this has been sort
of one of the real breakthroughs in

254
00:13:49,816 --> 00:13:53,350
algorithm design, mostly in the 70s,
realizing how important this is.

255
00:13:53,350 --> 00:13:58,220
That the use of randomization can make
algorithms more elegant, simpler,

256
00:13:58,220 --> 00:14:02,890
easier to code, more faster, or just
simply you can solve problems that you

257
00:14:02,890 --> 00:14:07,100
could not solve or at least not solve as
easily without the use of randomization.

258
00:14:07,100 --> 00:14:09,946
So it's really one thing that
should be in your toolbox as

259
00:14:09,946 --> 00:14:12,095
an algorithm designer, randomization.

260
00:14:12,095 --> 00:14:14,375
QuickSort will be the first cal,
killer application but

261
00:14:14,375 --> 00:14:16,190
we'll see a couple more
later in the course.

262
00:14:18,415 --> 00:14:20,449
Now by the end of this sequence of videos,

263
00:14:20,449 --> 00:14:24,014
I'll have given you a complete rigorous
argument about why this works.

264
00:14:24,014 --> 00:14:28,328
Why with random pivots, QuickSort
always runs very quickly on average.

265
00:14:28,328 --> 00:14:30,116
But, you know, before moving into sa,

266
00:14:30,116 --> 00:14:33,220
into anything too formal,
let's develop a little bit of intuition or

267
00:14:33,220 --> 00:14:36,654
at least kind of a day dream about why
on earth could this could possibly work?

268
00:14:36,654 --> 00:14:41,070
Why on earth this could possibly be a good
idea to have randomness internal to

269
00:14:41,070 --> 00:14:43,008
our QuickSort implementation.

270
00:14:43,008 --> 00:14:47,116
Well, so first, just you know, very high
level, what would be sort of the hope or

271
00:14:47,116 --> 00:14:47,903
the dream?

272
00:14:47,903 --> 00:14:50,769
The hope would be you know, random pivots,
they're not going to be perfect.

273
00:14:50,769 --> 00:14:52,927
I mean, you're not going to
just sort of guess the median.

274
00:14:52,927 --> 00:14:55,787
You only have a one in n chance of
figuring out which one the median is.

275
00:14:55,787 --> 00:14:59,968
But, the hope is that most choices
of a pivot will be good enough.

276
00:15:01,845 --> 00:15:02,979
So that's pretty fuzzy.

277
00:15:02,979 --> 00:15:05,746
Let's drill down a little bit, and
develop this intuition further.

278
00:15:06,786 --> 00:15:08,459
Let me describe it in two steps.

279
00:15:12,046 --> 00:15:16,079
The first claim is that, you know, in our
last quiz we said suppose we get lucky and

280
00:15:16,079 --> 00:15:18,960
we always pick the median in
every single recursive call.

281
00:15:18,960 --> 00:15:21,369
And we observe that we do great,
we get n log n running time.

282
00:15:21,369 --> 00:15:24,394
So now let's observe that actually
to get the n log n running time,

283
00:15:24,394 --> 00:15:28,510
it's not important that we magically get
the median every single recursive call.

284
00:15:28,510 --> 00:15:32,550
If we get any kind of reasonable pivot, by
which a pivot that gives us some kind of

285
00:15:32,550 --> 00:15:34,980
approximately balanced
split of the problems.

286
00:15:34,980 --> 00:15:36,550
Again, we're going to be good.

287
00:15:36,550 --> 00:15:37,460
Okay?
So the last quiz

288
00:15:37,460 --> 00:15:40,420
wasn't really particular to
getting the exact median.

289
00:15:40,420 --> 00:15:42,590
Near medians are also fine.

290
00:15:42,590 --> 00:15:48,168
To be concrete, suppose we always pick
a pivot which guarantees us a split of 25,

291
00:15:48,168 --> 00:15:49,295
75 or better.

292
00:15:49,295 --> 00:15:54,099
That is, both recursive calls should be
called on a raise of size at most 75% of

293
00:15:54,099 --> 00:15:55,879
the one that we started with.

294
00:15:57,583 --> 00:16:02,912
So precisely, if we always get a 25, 75
split or better in every recursive call, I

295
00:16:02,912 --> 00:16:07,600
claim that the running time of quick sort
in that event will be big O of n log n.

296
00:16:07,600 --> 00:16:10,680
Just like it was in the last quiz where
we were actually assuming something much

297
00:16:10,680 --> 00:16:12,560
stronger that we're getting the median.

298
00:16:12,560 --> 00:16:14,352
Now this is not so obvious.

299
00:16:14,352 --> 00:16:17,909
The fact that 25,
75 splits guaranteed n log n running time.

300
00:16:17,909 --> 00:16:20,706
For those of you that are feeling keen,
you might want to try to prove this.

301
00:16:20,706 --> 00:16:23,021
So you can prove this using
a recursion tree argument,

302
00:16:23,021 --> 00:16:25,739
that because you don't have
balanced sub-problems you have to

303
00:16:25,739 --> 00:16:29,032
work a little bit harder than you do in
the cases covered by the master method.

304
00:16:30,630 --> 00:16:32,480
So that's the first part of the intuition.

305
00:16:32,480 --> 00:16:34,938
And this is what we mean by
a pivot being good enough.

306
00:16:34,938 --> 00:16:37,920
If we get a 25, 75 split or
better, we're good to go.

307
00:16:37,920 --> 00:16:41,402
We get our desired,
our target n log n running time.

308
00:16:41,402 --> 00:16:44,899
So the second part of the intuition
is to realize that actually,

309
00:16:44,899 --> 00:16:48,937
we don't have to get all that lucky
to just be getting a 25, 75 split.

310
00:16:48,937 --> 00:16:50,427
That's actually a pretty modest goal.

311
00:16:50,427 --> 00:16:53,250
And even this modest goal's enough
to get the n log n running time.

312
00:16:53,250 --> 00:16:55,750
Right?
So suppose our array contains the numbers,

313
00:16:55,750 --> 00:16:57,750
the integers between one and 100.

314
00:16:57,750 --> 00:16:59,559
So it's an array of length 100.

315
00:16:59,559 --> 00:17:00,897
Think for a second,

316
00:17:00,897 --> 00:17:06,263
which of those elements is going to
give us a split that's 25, 75 or better?

317
00:17:07,569 --> 00:17:15,380
So, if we pick any element between 26 and
75, inclusive will be totally good.

318
00:17:15,380 --> 00:17:17,980
Right?
If we pick something that's at least 26,

319
00:17:17,980 --> 00:17:22,130
that means the less sub problem is going
to have at least the element 1 through 25,

320
00:17:22,130 --> 00:17:24,620
that have at least 25% of the elements.

321
00:17:24,620 --> 00:17:28,415
If we pick something less than 75, then
the right sub problem will have all of

322
00:17:28,415 --> 00:17:31,072
the elements 76 through
100 after we partition.

323
00:17:31,072 --> 00:17:33,909
So that will also have at
least 25% of the elements.

324
00:17:33,909 --> 00:17:39,470
So anything between 26 and
75 gives us a 75, 25 split or better.

325
00:17:39,470 --> 00:17:42,237
But that's a full half of the elements.

326
00:17:42,237 --> 00:17:45,897
So it's as good as just flipping
a fair coin and hoping we get heads.

327
00:17:45,897 --> 00:17:49,821
So with 50% probability,
we get a split good enough to get the,

328
00:17:49,821 --> 00:17:52,004
good enough to get this n log n bound.

329
00:17:52,004 --> 00:17:55,140
And so again, the high level hope
is that often enough, you know,

330
00:17:55,140 --> 00:17:58,057
half of the time, we get these you know,
good enough splits.

331
00:17:58,057 --> 00:17:59,685
25, 75 splits or better.

332
00:17:59,685 --> 00:18:04,081
So that would seem to suggest an n
log n running time on average is a,

333
00:18:04,081 --> 00:18:05,426
a legitimate hope.

334
00:18:07,482 --> 00:18:08,960
So that's the high level intuition.

335
00:18:08,960 --> 00:18:12,817
But if I were you I would certainly not
be content with this somewhat hand wavy

336
00:18:12,817 --> 00:18:14,938
explanation that I've given you so far.

337
00:18:14,938 --> 00:18:16,421
What I've told you is sort of the hope.

338
00:18:16,421 --> 00:18:17,317
The dream.

339
00:18:17,317 --> 00:18:20,700
Why there is at least
a chance this might work.

340
00:18:20,700 --> 00:18:22,707
But, but the question remains, and

341
00:18:22,707 --> 00:18:27,552
I would encourage such skepticism,
which is, does this really work?

342
00:18:27,552 --> 00:18:30,935
And to answer that, we're going to have
to do some actual mathematical analysis.

343
00:18:30,935 --> 00:18:31,720
And that's what I'm going to show you.

344
00:18:31,720 --> 00:18:32,690
I'm going to show you a complete,

345
00:18:32,690 --> 00:18:35,860
rigorous analysis of the quick sort
algorithm with random pivots and

346
00:18:35,860 --> 00:18:39,510
we'll show that, we'll show that yes,
in fact it does really work.

347
00:18:39,510 --> 00:18:42,250
And this highlights what's going to be
a recurring theme in this course and

348
00:18:42,250 --> 00:18:43,790
a recurring theme just in the study and

349
00:18:43,790 --> 00:18:46,900
understanding of algorithms,
which is its quite often there's some

350
00:18:46,900 --> 00:18:49,410
fundamental problem when you're
trying to code a solution, and

351
00:18:49,410 --> 00:18:53,400
you come up with a novel idea,
it might be brilliant, and it might suck.

352
00:18:53,400 --> 00:18:54,495
And you have no idea.

353
00:18:54,495 --> 00:18:57,768
Now obviously you can code up the idea,
run it on some concrete instances and

354
00:18:57,768 --> 00:19:00,550
get a feel for you know,
whether it seems like a good idea or not.

355
00:19:00,550 --> 00:19:04,014
But if you really want to know
fundamentally what makes the idea good or

356
00:19:04,014 --> 00:19:07,835
what makes the idea bad, really you
need to turn to mathematical analysis to

357
00:19:07,835 --> 00:19:09,581
give you a complete explanation.

358
00:19:09,581 --> 00:19:12,416
And that's exactly what we're
going to do with QuickSort, and

359
00:19:12,416 --> 00:19:14,786
it will explain in a very deep
way why it works so well.

360
00:19:16,380 --> 00:19:19,003
Specifically in the next
sequence of three videos,

361
00:19:19,003 --> 00:19:23,367
I'm going to show you an analysis, a proof
of the following theorem about QuickSort.

362
00:19:25,075 --> 00:19:27,816
So under no assumptions about the data,
that is for

363
00:19:27,816 --> 00:19:30,060
every input array of a given length say n.

364
00:19:31,616 --> 00:19:35,383
The average running time of Quicksort,

365
00:19:35,383 --> 00:19:40,272
implemented with random pivots,
is big O of n log n.

366
00:19:40,272 --> 00:19:42,072
And again in fact it's theta of n log n,

367
00:19:42,072 --> 00:19:44,265
but we'll just focus on
the big O of n log n part.

368
00:19:45,296 --> 00:19:50,102
So this is a very, very cool theorem about
this randomized QuickSort algorithm.

369
00:19:50,102 --> 00:19:51,962
One thing I want to be clear, so

370
00:19:51,962 --> 00:19:56,320
that you don't undersell this
guarantee in your own mind.

371
00:19:56,320 --> 00:19:59,680
This is a worse case guarantee
with respect to the input.

372
00:19:59,680 --> 00:20:02,586
Okay, so notice at the beginning
of this theorem what do we say?

373
00:20:02,586 --> 00:20:06,679
For every input array of length n,
all right?

374
00:20:06,679 --> 00:20:08,997
So we have absolutely no
assumptions about the data.

375
00:20:08,997 --> 00:20:13,224
This is a totally general purpose sorting
sub-routine which you can use whenever you

376
00:20:13,224 --> 00:20:16,290
want, even if you have no idea
where the data's coming from.

377
00:20:16,290 --> 00:20:18,635
And these guarantees
are still going to be true.

378
00:20:20,701 --> 00:20:25,237
This of course is something I held
forth about at some length back in

379
00:20:25,237 --> 00:20:27,427
our guiding principles video.

380
00:20:27,427 --> 00:20:30,456
When I argued that if you can get
away with it, what you really want is

381
00:20:30,456 --> 00:20:33,330
general purpose algorithms,
which make no data assumptions so

382
00:20:33,330 --> 00:20:36,790
they can be used over and over again
in all kinds of different contexts.

383
00:20:36,790 --> 00:20:38,870
And that still have great guarantees.

384
00:20:38,870 --> 00:20:40,270
And QuickSort is one of those.

385
00:20:40,270 --> 00:20:43,880
So basically if you have a dataset and it
fits in the main memory of your machine,

386
00:20:43,880 --> 00:20:47,550
again sorting is a for-free sub-routine,
in particular, QuickSort.

387
00:20:47,550 --> 00:20:49,500
The QuickSort of limitation is for free.

388
00:20:49,500 --> 00:20:51,490
So this just runs so blazingly fast.

389
00:20:51,490 --> 00:20:53,018
It doesn't matter what the array is,

390
00:20:53,018 --> 00:20:55,951
maybe you don't even know why you
want to sort it, but go ahead, why not?

391
00:20:55,951 --> 00:20:59,412
Maybe it'll make your life easier like
it did for example in the closest pair

392
00:20:59,412 --> 00:21:02,514
algorithm for those of you that
watched those two optional videos.

393
00:21:03,896 --> 00:21:07,379
Now the word average does
appear in this theorem.

394
00:21:07,379 --> 00:21:09,132
And, you know, as I've been harping on,

395
00:21:09,132 --> 00:21:11,340
this average is not over any
assumptions on the data.

396
00:21:11,340 --> 00:21:14,360
We're certainly not assuming that
the input array is random in any sense.

397
00:21:14,360 --> 00:21:15,880
The input array can be anything.

398
00:21:15,880 --> 00:21:17,600
So where is the averaging coming from?

399
00:21:17,600 --> 00:21:19,990
The averaging is coming
only from randomness,

400
00:21:19,990 --> 00:21:22,350
which is internal to our algorithm.

401
00:21:22,350 --> 00:21:25,170
Randomness that we put
in the code ourselves.

402
00:21:25,170 --> 00:21:26,481
That we're responsible for.

403
00:21:28,539 --> 00:21:31,947
So remember, randomized algorithms have
the interesting property that even if

404
00:21:31,947 --> 00:21:33,398
you run it on the same input over and

405
00:21:33,398 --> 00:21:35,770
over again,
you're going to get different executions.

406
00:21:35,770 --> 00:21:38,680
So the running time of a randomized
algorithm depend you know,

407
00:21:38,680 --> 00:21:41,640
can vary as you run it on the same
input over and over again.

408
00:21:41,640 --> 00:21:44,460
The quizzes have taught us that
the running time of QuickSort on

409
00:21:44,460 --> 00:21:48,080
a given input, fluctuates from
anywhere between the best case of

410
00:21:48,080 --> 00:21:50,660
n log n to the worst case of n squared.

411
00:21:50,660 --> 00:21:55,680
So what this theorem is telling us, is
that for every possible input array, while

412
00:21:55,680 --> 00:21:59,080
the running time does indeed fluctuate
between an upper bound of n squared and

413
00:21:59,080 --> 00:22:02,770
the lower bound of n log n,
the best case is dominating.

414
00:22:02,770 --> 00:22:03,990
On average it's n log n.

415
00:22:03,990 --> 00:22:07,470
On average it's almost as
good as the best case.

416
00:22:07,470 --> 00:22:09,140
That's what's so amazing about QuickSort.

417
00:22:09,140 --> 00:22:13,080
That was, in a square that can pop up
once in a while has, doesn't matter.

418
00:22:13,080 --> 00:22:14,300
You're never going to see it.

419
00:22:14,300 --> 00:22:17,330
You're always going to see this n log n
like behavior in randomized QuickSort.

420
00:22:17,330 --> 00:22:17,920
So for

421
00:22:17,920 --> 00:22:21,440
some of you I'll see you next in a video
on probability review, that's optional.

422
00:22:21,440 --> 00:22:23,860
For the rest of you, I'll see you
in the analysis of this theorem.

